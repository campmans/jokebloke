{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebe9f644",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import wave\n",
    "import speech_recognition as sr\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44a8f123",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ca6e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import Literal\n",
    "from abc import ABC, abstractmethod\n",
    "import subprocess\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "class LLMJokerAgent(ABC):\n",
    "    \"\"\"Base class for LLM-powered joke generation agents.\"\"\"\n",
    "\n",
    "    MEMORY_SYSTEM_INSTRUCTION = \"You are an advanced agent memory manager that keeps track of conversation content by maintaining a SHORT summary. You transform the old summary, with the new user message and agent response to the new summary.\"\n",
    "\n",
    "    def __init__(self, prompts_dir: Path):\n",
    "        self.prompts_dir = Path(prompts_dir)\n",
    "        self.memory = \"\"\n",
    "\n",
    "        # Load output format\n",
    "        output_format_path = self.prompts_dir / \"output-format.md\"\n",
    "        with open(output_format_path, 'r') as f:\n",
    "            self.output_format = f.read()\n",
    "\n",
    "        # Load profile prompts into a dictionary\n",
    "        self.profiles = {}\n",
    "        for file in self.prompts_dir.iterdir():\n",
    "            if file.suffix == '.md' and 'output' not in file.name:\n",
    "                with open(file, 'r') as f:\n",
    "                    self.profiles[file.stem] = f.read()\n",
    "\n",
    "        self.profile_names = list(self.profiles.keys())\n",
    "\n",
    "    @abstractmethod\n",
    "    def _call_llm(self, system_instruction: str, user_content: str) -> str | None:\n",
    "        \"\"\"Sub-classes implement this to either prompt with the CLI or the API\"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_random_profile(self):\n",
    "        name = np.random.choice(self.profile_names)\n",
    "        return name, self.profiles[name]\n",
    "\n",
    "    def generate_response(self, user_response: str = \"I don't have much to say.\", personality=None, update_memory=True, N=3):\n",
    "        \"\"\"Generate a joke with a random personality if personaltiy is not set.\"\"\"\n",
    "        _, profile = self.get_random_profile()\n",
    "        \n",
    "        if personality is not None:\n",
    "            # Soft fail if personality does not exist\n",
    "            profile = self.profiles.get(personality, profile)\n",
    "        \n",
    "        system_instruction = f\"{profile}\\n{self.output_format.replace('[N]', str(N))}\"\n",
    "\n",
    "        model_response = self._call_llm(system_instruction, user_response)\n",
    "        \n",
    "        if not model_response:\n",
    "            return [\"I'm done for the day\"]\n",
    "        \n",
    "        try:\n",
    "            parsed_response = ET.fromstring(model_response)\n",
    "            response_jokes = list(map(lambda x: x.text, parsed_response.findall(\"joke\")))\n",
    "            if response_jokes is None or response_jokes[0] is None:\n",
    "                return [\"I'm not feeling so funny today.\"]\n",
    "            if update_memory:\n",
    "                self.update_memory(user_response, response_jokes[0])\n",
    "        except:\n",
    "            return [\"XML in this case stands for X Major Loser-markup, because I cannot parse it.\"]\n",
    "\n",
    "        return response_jokes\n",
    "\n",
    "    # TODO: we might want to handle this completely differently with another agent without an LLM\n",
    "    def update_memory(self, user_response: str, model_response: str) -> str:\n",
    "        \"\"\"Update conversation memory with a summary of the exchange.\"\"\"\n",
    "        prompt = f\"previous summary: {self.memory}, new user msg: {user_response}, new system msg: {model_response}\"\n",
    "\n",
    "        summary = self._call_llm(self.MEMORY_SYSTEM_INSTRUCTION, prompt)\n",
    "\n",
    "        if summary:\n",
    "            self.memory = summary\n",
    "            print(\"New memory:\\n\", self.memory)\n",
    "\n",
    "        return self.memory\n",
    "\n",
    "class LLMAPIJokerAgent(LLMJokerAgent):\n",
    "    \"\"\"Implementation with the gemini API\"\"\"\n",
    "    # Default to Jelle's API key\n",
    "    def __init__(self, prompts_dir: Path, api_key: str=\"AIzaSyC2B_9Koiklo6Dh5WsxtZe7J7iU2ZFp01Q\", model=\"gemini-2.5-flash\"):\n",
    "        super().__init__(prompts_dir)\n",
    "        self.client = genai.Client(api_key=api_key)\n",
    "        self.model = model\n",
    "\n",
    "    def _call_llm(self, system_instruction: str, user_content: str) -> str | None:\n",
    "        response = self.client.models.generate_content(\n",
    "            model=self.model,\n",
    "            config=types.GenerateContentConfig(\n",
    "                system_instruction=system_instruction\n",
    "            ),\n",
    "            contents=user_content,\n",
    "        )\n",
    "        return response.text\n",
    "\n",
    "# To use this, you have to get your free student gemini pro, and install the CLI.\n",
    "class LLMCLIJokerAgent(LLMJokerAgent):\n",
    "    \"\"\"Implementation with the gemini CLI\"\"\"\n",
    "\n",
    "    def __init__(self, prompts_dir: Path):\n",
    "        super().__init__(prompts_dir)\n",
    "\n",
    "    def _call_llm(self, system_instruction: str, user_content: str) -> str | None:\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"gemini\", \"-p\", f\"{system_instruction}\\n\\nUser: {user_content}\"],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=60\n",
    "            )\n",
    "\n",
    "            if result.returncode != 0:\n",
    "                print(f\"CLI Error: {result.stderr}\")\n",
    "                return None\n",
    "\n",
    "            return result.stdout.strip() or None\n",
    "\n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"CLI timeout\")\n",
    "            return None\n",
    "        except FileNotFoundError:\n",
    "            print(\"Gemini CLI not found.\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf5a14be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I had to take a test once. The only question was \"What is the capital of pancakes?\" I answered \"The sky is made of retired librarians.\" The teacher marked it correct and then I woke up inside a filing cabinet.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joker_agent = LLMCLIJokerAgent(os.getcwd()/Path(\"prompts\"))\n",
    "joker_agent.generate_response(\"I'm just testing you out if you don't mind.\", update_memory=False, N=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6675d7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class MemoryAgent:\n",
    "    # We use these to track when the user refers back to nouns that were said before.\n",
    "    PRONOUNS = {\"it\", \"they\", \"he\", \"she\", \"this\", \"that\", \"these\", \"those\", \"i\", \"me\", \"my\", \"we\", \"us\", \"our\", \"you\", \"your\"}\n",
    "\n",
    "    def __init__(self, max_memory_length):\n",
    "        self.memory = []\n",
    "        self.max_memory_length = max_memory_length\n",
    "\n",
    "    def user_update(self, content: str):\n",
    "        features = self._extract_features(content)\n",
    "        if self.memory and any(s.lower() in self.PRONOUNS for s in features[\"subj\"]):\n",
    "            prev = self.memory[-1]\n",
    "            features[\"resolved_refs\"] = prev.get(\"subj\", []) + prev.get(\"obj\", []) + prev.get(\"entities\", [])\n",
    "        self.memory.append(features)\n",
    "        if len(self.memory) > self.max_memory_length:\n",
    "            self.memory = self.memory[-self.max_memory_length:]\n",
    "\n",
    "    def get_full_memory_summary(self, n_contrast: int = 1) -> str:\n",
    "        if not self.memory:\n",
    "            return \"\"\n",
    "\n",
    "        current_idx = len(self.memory) - 1\n",
    "        relevant_idx = self._get_relevant_indices(current_idx)\n",
    "        non_relevant_idx = [i for i in range(current_idx + 1) if i not in relevant_idx]\n",
    "\n",
    "        # Group non-relevant by topic and randomly select n\n",
    "        contrast_groups = []\n",
    "        used = set()\n",
    "        for idx in sorted(non_relevant_idx, reverse=True):\n",
    "            if idx in used:\n",
    "                continue\n",
    "            group = [i for i in self._get_relevant_indices(idx) if i in non_relevant_idx]\n",
    "            if group:\n",
    "                contrast_groups.append(group)\n",
    "                used.update(group)\n",
    "\n",
    "        selected_contrasts = random.sample(contrast_groups, min(n_contrast, len(contrast_groups)))\n",
    "\n",
    "        parts = [self._summarize([self.memory[i] for i in relevant_idx])]\n",
    "        for group in selected_contrasts:\n",
    "            parts.append(self._summarize([self.memory[i] for i in group]))\n",
    "\n",
    "        return \" | \".join(filter(None, parts))\n",
    "\n",
    "    def _get_relevant_indices(self, target_idx: int) -> set[int]:\n",
    "        if target_idx < 0 or target_idx >= len(self.memory):\n",
    "            return set()\n",
    "        target_refs = self._get_refs(self.memory[target_idx])\n",
    "        return {i for i in range(target_idx + 1) if not target_refs.isdisjoint(self._get_refs(self.memory[i]))}\n",
    "\n",
    "    def _get_refs(self, f: dict) -> set:\n",
    "        all_refs = f.get(\"subj\", []) + f.get(\"obj\", []) + f.get(\"entities\", []) + f.get(\"resolved_refs\", [])\n",
    "        return {r for r in all_refs if r.lower() not in self.PRONOUNS}\n",
    "\n",
    "    def _summarize(self, memory_list: list[dict]) -> str:\n",
    "        if not memory_list:\n",
    "            return \"\"\n",
    "        tuples = []\n",
    "        for f in memory_list:\n",
    "            tuples.append((\n",
    "                \" \".join(w for w in f.get(\"subj\", []) if w.lower() not in self.PRONOUNS),\n",
    "                \" \".join(f.get(\"verbs\", [])),\n",
    "                \" \".join(f.get(\"adjectives\", [])),\n",
    "                \" \".join(w for w in f.get(\"obj\", []) if w.lower() not in self.PRONOUNS),\n",
    "                \" \".join(f.get(\"entities\", [])),\n",
    "            ))\n",
    "        # We basically create a string with subjects, verbs, adjectives, ... with this zip.\n",
    "        return \" \".join(filter(None, (\" \".join(filter(None, t)) for t in zip(*tuples))))\n",
    "\n",
    "    def _extract_features(self, sentence: str) -> dict:\n",
    "        doc = nlp(sentence)\n",
    "        return {\n",
    "            \"entities\": [e.text for e in doc.ents],\n",
    "            \"subj\": [t.text for t in doc if \"subj\" in t.dep_],\n",
    "            \"obj\": [t.text for t in doc if \"obj\" in t.dep_],\n",
    "            \"adjectives\": [t.text for t in doc if t.pos_ == \"ADJ\"],\n",
    "            \"verbs\": [t.lemma_ for t in doc if t.pos_ == \"VERB\"],\n",
    "            \"past_tense\": any(t.tag_ == \"VBD\" for t in doc),\n",
    "            \"negated\": any(t.dep_ == \"neg\" for t in doc),\n",
    "            \"numbers\": [t.text for t in doc if t.like_num],\n",
    "        }\n",
    "\n",
    "memory = MemoryAgent(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afaacb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wave\n",
    "import speech_recognition as sr\n",
    "from io import BytesIO\n",
    "import pyttsx3\n",
    "\n",
    "# ----------------------------\n",
    "# Convert recorder audio to WAV in-memory\n",
    "# ----------------------------\n",
    "def get_audio_as_wav_bytes(recorder):\n",
    "    \"\"\"Return recorder audio as a WAV byte stream.\"\"\"\n",
    "    data = recorder.audiodata\n",
    "    srate = recorder.sampleRate\n",
    "    \n",
    "    if data is None or len(data) == 0:\n",
    "        raise ValueError(\"No audio recorded!\")\n",
    "\n",
    "    # Convert float32 [-1,1] → int16\n",
    "    data_int16 = (data * 32767).astype(np.int16)\n",
    "\n",
    "    # Write WAV to BytesIO\n",
    "    wav_bytes = BytesIO()\n",
    "    with wave.open(wav_bytes, 'wb') as wf:\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(2)\n",
    "        wf.setframerate(srate)\n",
    "        wf.writeframes(data_int16.tobytes())\n",
    "    wav_bytes.seek(0)\n",
    "    return wav_bytes\n",
    "\n",
    "# ----------------------------\n",
    "# Transcribe using speech_recognition\n",
    "# ----------------------------\n",
    "recognizer = sr.Recognizer()\n",
    "def transcribe(recorder):\n",
    "    global recognizer\n",
    "    wav_bytes = get_audio_as_wav_bytes(recorder)\n",
    "\n",
    "    with sr.AudioFile(wav_bytes) as source:\n",
    "        audio = recognizer.record(source)\n",
    "    \n",
    "    # You can use different recognizers, here we use Google free API\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio)\n",
    "        print(\"Transcription:\", text)\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Could not understand audio\")\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"API error: {e}\")\n",
    "    return None\n",
    "\n",
    "#Text to speech\n",
    "engine = None\n",
    "def SpeakText(command):\n",
    "    # Initialize the engine\n",
    "    global engine\n",
    "    if engine is None:\n",
    "        engine = pyttsx3.init()\n",
    "    \n",
    "    engine.say(command) \n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "499d270d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Plot and Save audio (optional)\n",
    "def save_wav_from_recorder(recorder, filename=\"recording.wav\"):\n",
    "    \"\"\"Save the recorded audio from recorder into a WAV file.\"\"\"\n",
    "    data = recorder.audiodata\n",
    "    sr = recorder.sampleRate\n",
    "    \n",
    "    if data is None or len(data) == 0:\n",
    "        print(\"No audio data to save!\")\n",
    "        return None\n",
    "\n",
    "    # Convert float32 [-1,1] → int16\n",
    "    data_int16 = (data * 32767).astype(np.int16)\n",
    "\n",
    "    with wave.open(filename, \"wb\") as f:\n",
    "        f.setnchannels(1)\n",
    "        f.setsampwidth(2)  # int16\n",
    "        f.setframerate(sr)\n",
    "        f.writeframes(data_int16.tobytes())\n",
    "\n",
    "    print(f\"Saved: {filename}\")\n",
    "    return filename\n",
    "\n",
    "def plot_audio(rec):        \n",
    "    # Plot waveform\n",
    "    plt.figure(figsize=(10,3))\n",
    "    plt.plot(rec.audiodata)\n",
    "    plt.title(\"Recorded Audio Waveform\")\n",
    "    plt.xlabel(\"Samples\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65218f4a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a23ef7ae31499b804b50dfc2cc6638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "AudioRecorder()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90915424c23541e088f897d3ce722178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid black', border_left='1px solid black', border_right='1px solid b…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Audio Recorder Notebook\n",
    "# ----------------------------\n",
    "\n",
    "import numpy as np\n",
    "import wave\n",
    "import ipyaudioworklet as ipyaudio\n",
    "from ipywidgets import Output, VBox\n",
    "from IPython.display import display\n",
    "\n",
    "# ----------------------------\n",
    "# Create Recorder\n",
    "# ----------------------------\n",
    "rec = ipyaudio.AudioRecorder()\n",
    "display(rec)\n",
    "\n",
    "# ----------------------------\n",
    "# Status Output\n",
    "# ----------------------------\n",
    "status_out = Output(layout={'border': '1px solid black', 'padding': '5px'})\n",
    "status_out.append_stdout(\"Recorder ready.\\n\")\n",
    "display(status_out)\n",
    "\n",
    "@status_out.capture(clear_output=True)\n",
    "def status_changed(change):\n",
    "    print(\"Status:\", change.new)\n",
    "\n",
    "rec.observe(status_changed, \"status\")\n",
    "\n",
    "# ----------------------------\n",
    "# Automatic Save and Plot\n",
    "# ----------------------------\n",
    "@status_out.capture()\n",
    "def on_status_change(change):\n",
    "    if change.new in (\"STOPPED\", \"RECORDED\"):\n",
    "        \n",
    "        user_input = transcribe(rec)\n",
    "        \n",
    "        if user_input is None:\n",
    "            return\n",
    "        \n",
    "        memory.user_update(user_input)\n",
    "        \n",
    "        # TODO: block all user input during this step\n",
    "        # TODO: say something random and funny every couple of seconds to indicate that you're thinking about a response\n",
    "        # One jokes already takes approx. 15 seconds.\n",
    "        jokes = joker_agent.generate_response(f\"<history>{memory.get_full_memory_summary(2)}</history>\\n{user_input}\", update_memory=False, N=1)\n",
    "        SpeakText(jokes[0])\n",
    "\n",
    "rec.observe(on_status_change, \"status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9736d75",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee020a039eb4f15a58d7160e162a711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Do you like this Answer?')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "483a04b95fa443ba8d74089947aa99f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description=': )', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "520dd22a6a254295b801d663c08e69a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='danger', description=': (', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b33b286b7eba41f6ac71b59843d8c9a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "text_to_display = \"Do you like this Answer?\"\n",
    "text_label = widgets.Label(value=text_to_display)\n",
    "\n",
    "like_button = widgets.Button(description=\": )\", button_style='success')\n",
    "dislike_button = widgets.Button(description=\": (\", button_style='danger')\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_like_clicked(b):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        print(\"You liked this!\")\n",
    "\n",
    "def on_dislike_clicked(b):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        SpeakText(\"Wow, tough crowd!\")\n",
    "        print(\"You disliked this!\")\n",
    "\n",
    "like_button.on_click(on_like_clicked)\n",
    "dislike_button.on_click(on_dislike_clicked)\n",
    "\n",
    "display(text_label, like_button, dislike_button, output)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "jokebloke",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
